{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion detection from text using PyTorch and Federated Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankishinani/Emotion-detection-from-text-using-PyTorch-and-Federated-Learning/blob/master/Emotion_detection_from_text_using_PyTorch_and_Federated_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rufFNufqDmUO",
        "colab_type": "text"
      },
      "source": [
        "# Emotion detection from text using PyTorch and Federated Learning\n",
        "\n",
        "For this project, we are going to implement an NLP task of creating a model to detect the emotion from text. We will develop this using the PyTorch library and the Federated Learning framework for decentralized training. \n",
        "\n",
        "We will create an emotion detection for the following 5 emotions:\n",
        "\n",
        "| Emotion | Emoji   | Label   |\n",
        "|------|------|------|\n",
        "|Loving| ❤️| 0|\n",
        "|Playful| ⚽️| 1|\n",
        "|Happy| 😄| 2|\n",
        "|Annoyed| 😞| 3|\n",
        "|Foodie| 🍽| 4|\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will work with a dataset (X, Y) where we have:\n",
        "*   X contains 127 sentences\n",
        "*   Y contains a label between [0, 4] corresponding to the emotion\n",
        "\n",
        "For example:\n",
        "\n",
        "| Sentence | Emotion   |\n",
        "|----------|-----------|\n",
        "|food is life|  🍽 Foodie|\n",
        "|I love you mum|  ❤️ Loving|\n",
        "|Stop saying bullshit|  😞 Annoyed|\n",
        "|congratulations on your acceptance|  😄 Happy|\n",
        "|The assignment is too long|    😞 Annoyed|\n",
        "|I want to go play| ⚽️ Playful|\n",
        "|she did not answer my text| 😞 Annoyed|\n",
        "|Your stupidity has no limit| 😞 Annoyed|\n",
        "|how many points did he score|  ⚽️ Playful|\n",
        "|my algorithm performs poorly| 😞 Annoyed|\n",
        "|I got approved|  😄 Happy|\n",
        "\n",
        "## The Model\n",
        "We will build an LSTM model that takes as input word sequences that will take word ordering into account. We will use 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained word embeddings to represent words. We will then feed those as an input into an LSTM that will predict the most appropiate emotion for the text. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1s-KYhU5JWF-jvAlZ2MIKKugxLLDdhpQP)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQn1wO2qr01C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJcTk7DnfXRx",
        "colab_type": "code",
        "outputId": "98b40cb6-9ceb-4b4e-df3b-358bc19bf637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 189
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04n5XXyf5Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "\n",
        "def read_csv(filename):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmqjSlwofS_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = read_csv('/content/drive/My Drive/Colab Notebooks/data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('/content/drive/My Drive/Colab Notebooks/data/tesss.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt5z5eqVih4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDe5vX3qiYcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/My Drive/Colab Notebooks/data/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh0iyxn_jDCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]  # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
        "    X_indices = np.zeros((m,max_len))\n",
        "    \n",
        "    for i in range(m):  # loop over training examples\n",
        "        \n",
        "        # Convert the ith sentence in lower case and split into a list of words\n",
        "        sentence_words = X[i].lower().split()\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j = j + 1\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLyIs_aFjdfl",
        "colab_type": "code",
        "outputId": "6d598bf1-3e31-4ff4-90e2-53847adfbe59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X1 = np.array([\"lol\", \"I love you\", \"this is very yummy\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['lol' 'I love you' 'this is very yummy']\n",
            "X1_indices = [[225122.      0.      0.      0.      0.]\n",
            " [185457. 226278. 394475.      0.      0.]\n",
            " [358160. 192973. 377946. 394957.      0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx57hw8Pjg3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMKHhghrWcNn",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Network using Pretrained Embedding Layer using GloVe Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZDWRfRkWhwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NN(nn.Module):\n",
        "  '''\n",
        "    def __init__(self, maxLen, word_to_vec_map, word_to_index):\n",
        "        super(NN, self).__init__()\n",
        "        \n",
        "        self.embedding, embed_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, True)\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 1\n",
        "        self.output_size = 5\n",
        "        self.lstm = nn.LSTM(maxLen, maxLen)\n",
        "        self.fc1 = nn.Linear(maxLen, self.output_size)\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        x = self.embedding(sentence.type(torch.LongTensor))\n",
        "        print(x)\n",
        "        x, _ = self.lstm(x, self.hidden_size)\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x, _ = self.lstm(x, self.hidden_size)\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.fc1(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "  '''\n",
        "  def __init__(self, embedding, embedding_dim, hidden_dim, vocab_size, output_dim, batch_size):\n",
        "      super(NN, self).__init__()\n",
        "\n",
        "      self.batch_size = batch_size\n",
        "\n",
        "      self.hidden_dim = hidden_dim\n",
        "\n",
        "      self.word_embeddings = embedding\n",
        "\n",
        "      # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "      # with dimensionality hidden_dim.\n",
        "      self.lstm = nn.LSTM(embedding_dim, \n",
        "                          hidden_dim, \n",
        "                          num_layers=2,\n",
        "                          dropout = 0.5,\n",
        "                          batch_first = True)\n",
        "\n",
        "      # The linear layer that maps from hidden state space to output space\n",
        "      self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def forward(self, sentence):\n",
        "      \n",
        "      sentence = sentence.type(torch.LongTensor)\n",
        "      #print ('Shape of sentence is:', sentence.shape)\n",
        "\n",
        "      embeds = self.word_embeddings(sentence)\n",
        "      #print ('Embedding layer output shape', embeds.shape)\n",
        "\n",
        "      # initializing the hidden state to 0\n",
        "      hidden=None\n",
        "      lstm_out, h = self.lstm(embeds, hidden)\n",
        "      #print ('LSTM layer output shape', lstm_out.shape)\n",
        "      #print ('LSTM layer output ', lstm_out)\n",
        "\n",
        "      # Dropout\n",
        "      lstm_out = F.dropout(lstm_out, 0.5)\n",
        "\n",
        "      fc_out = self.fc(lstm_out.contiguous().view(-1, self.hidden_dim))\n",
        "      #print ('FC layer output shape', fc_out.shape)\n",
        "      #print ('FC layer output ', fc_out)\n",
        "      \n",
        "      out = fc_out.view(len(sentence), -1)\n",
        "      out = F.softmax(out, dim=1)\n",
        "      #print ('Output layer output shape', out.shape)\n",
        "      #print ('Output layer output ', out)\n",
        "      return out\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEkK7S6v8dVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "    \n",
        "    model.to(device)\n",
        "    running_loss = 0\n",
        "    \n",
        "    train_losses, test_losses = [], []\n",
        "    for e in range(epochs):\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        for sentences, labels in trainloader:\n",
        "\n",
        "            sentences, labels = sentences.to(device), labels.to(device)\n",
        "\n",
        "            # 1) erase previous gradients (if they exist)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2) make a prediction\n",
        "            pred = model.forward(sentences)\n",
        "\n",
        "            # 3) calculate how much we missed\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            # 4) figure out which weights caused us to miss\n",
        "            loss.backward()\n",
        "\n",
        "            # 5) change those weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # 6) log our progress\n",
        "            running_loss += loss.item()\n",
        "        \n",
        "        else:\n",
        "          test_loss = 0\n",
        "          accuracy = 0\n",
        "          \n",
        "          # Turn off gradients for validation, saves memory and computations\n",
        "          with torch.no_grad():\n",
        "              for images, labels in test_loader:\n",
        "                  log_ps = model(images)\n",
        "                  test_loss += criterion(log_ps, labels)\n",
        "                  \n",
        "                  ps = torch.exp(log_ps)\n",
        "                  top_p, top_class = ps.topk(1, dim=1)\n",
        "                  equals = top_class == labels.view(*top_class.shape)\n",
        "                  accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "                  \n",
        "          train_losses.append(running_loss/len(train_loader))\n",
        "          test_losses.append(test_loss/len(test_loader))\n",
        "\n",
        "          print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
        "                \"Training Loss: {:.3f}.. \".format(running_loss/len(train_loader)),\n",
        "                \"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
        "                \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-UcwaXPgZyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True):\n",
        "    num_embeddings = len(word_to_index) + 1                   \n",
        "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
        "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        weights_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    embed = nn.Embedding.from_pretrained(torch.from_numpy(weights_matrix).type(torch.FloatTensor), freeze=non_trainable)#(num_embeddings, embedding_dim)\n",
        "    # pretrained_weight is a numpy matrix of shape (num_embeddings, embedding_dim)\n",
        "    #embed.weight.data.copy_(torch.from_numpy(weights_matrix).type(torch.FloatTensor))\n",
        "\n",
        "    #emb_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix).type(torch.FloatTensor))\n",
        "\n",
        "    #if non_trainable:\n",
        "        #embed.weight.requires_grad = False\n",
        "\n",
        "    return embed, num_embeddings, embedding_dim\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgb8si4s9Jb9",
        "colab_type": "code",
        "outputId": "238565cb-7d54-47e9-d5cb-bf4c3ffb1870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "import torch.utils.data\n",
        "\n",
        "maxLen = len(max(X_train, key=len).split())\n",
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "#Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
        "\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "#Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "\n",
        "embedding, vocab_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True)\n",
        "\n",
        "hidden_dim=128\n",
        "output_size=1\n",
        "batch_size = 32\n",
        "\n",
        "print ('Embedding layer is ', embedding)\n",
        "print ('Embedding layer weights ', embedding.weight.shape)\n",
        "\n",
        "model = NN(embedding, embedding_dim, hidden_dim, vocab_size, output_size, batch_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "epochs = 50\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_indices), torch.tensor(Y_train).type(torch.LongTensor))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test_indices), torch.tensor(Y_test).type(torch.LongTensor))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, epochs)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding layer is  Embedding(400001, 50)\n",
            "Embedding layer weights  torch.Size([400001, 50])\n",
            "Epoch: 1/50..  Training Loss: 2.298..  Test Loss: 2.285..  Test Accuracy: 0.255\n",
            "Epoch: 2/50..  Training Loss: 4.560..  Test Loss: 2.222..  Test Accuracy: 0.177\n",
            "Epoch: 3/50..  Training Loss: 6.777..  Test Loss: 2.200..  Test Accuracy: 0.219\n",
            "Epoch: 4/50..  Training Loss: 8.978..  Test Loss: 2.186..  Test Accuracy: 0.188\n",
            "Epoch: 5/50..  Training Loss: 11.158..  Test Loss: 2.172..  Test Accuracy: 0.167\n",
            "Epoch: 6/50..  Training Loss: 13.322..  Test Loss: 2.151..  Test Accuracy: 0.328\n",
            "Epoch: 7/50..  Training Loss: 15.475..  Test Loss: 2.158..  Test Accuracy: 0.255\n",
            "Epoch: 8/50..  Training Loss: 17.590..  Test Loss: 2.157..  Test Accuracy: 0.286\n",
            "Epoch: 9/50..  Training Loss: 19.714..  Test Loss: 2.143..  Test Accuracy: 0.292\n",
            "Epoch: 10/50..  Training Loss: 21.819..  Test Loss: 2.116..  Test Accuracy: 0.432\n",
            "Epoch: 11/50..  Training Loss: 23.899..  Test Loss: 2.151..  Test Accuracy: 0.276\n",
            "Epoch: 12/50..  Training Loss: 25.931..  Test Loss: 2.123..  Test Accuracy: 0.422\n",
            "Epoch: 13/50..  Training Loss: 27.921..  Test Loss: 2.139..  Test Accuracy: 0.339\n",
            "Epoch: 14/50..  Training Loss: 29.874..  Test Loss: 2.108..  Test Accuracy: 0.365\n",
            "Epoch: 15/50..  Training Loss: 31.836..  Test Loss: 2.140..  Test Accuracy: 0.234\n",
            "Epoch: 16/50..  Training Loss: 33.758..  Test Loss: 2.128..  Test Accuracy: 0.312\n",
            "Epoch: 17/50..  Training Loss: 35.622..  Test Loss: 2.104..  Test Accuracy: 0.328\n",
            "Epoch: 18/50..  Training Loss: 37.463..  Test Loss: 2.105..  Test Accuracy: 0.333\n",
            "Epoch: 19/50..  Training Loss: 39.296..  Test Loss: 2.111..  Test Accuracy: 0.349\n",
            "Epoch: 20/50..  Training Loss: 41.126..  Test Loss: 2.101..  Test Accuracy: 0.370\n",
            "Epoch: 21/50..  Training Loss: 43.024..  Test Loss: 2.131..  Test Accuracy: 0.318\n",
            "Epoch: 22/50..  Training Loss: 44.811..  Test Loss: 2.067..  Test Accuracy: 0.411\n",
            "Epoch: 23/50..  Training Loss: 46.757..  Test Loss: 2.078..  Test Accuracy: 0.375\n",
            "Epoch: 24/50..  Training Loss: 48.557..  Test Loss: 2.101..  Test Accuracy: 0.359\n",
            "Epoch: 25/50..  Training Loss: 50.356..  Test Loss: 2.053..  Test Accuracy: 0.406\n",
            "Epoch: 26/50..  Training Loss: 52.241..  Test Loss: 2.058..  Test Accuracy: 0.370\n",
            "Epoch: 27/50..  Training Loss: 54.103..  Test Loss: 2.094..  Test Accuracy: 0.365\n",
            "Epoch: 28/50..  Training Loss: 55.946..  Test Loss: 2.005..  Test Accuracy: 0.469\n",
            "Epoch: 29/50..  Training Loss: 57.763..  Test Loss: 2.039..  Test Accuracy: 0.385\n",
            "Epoch: 30/50..  Training Loss: 59.558..  Test Loss: 2.095..  Test Accuracy: 0.365\n",
            "Epoch: 31/50..  Training Loss: 61.417..  Test Loss: 2.088..  Test Accuracy: 0.359\n",
            "Epoch: 32/50..  Training Loss: 63.205..  Test Loss: 2.062..  Test Accuracy: 0.417\n",
            "Epoch: 33/50..  Training Loss: 65.000..  Test Loss: 2.050..  Test Accuracy: 0.422\n",
            "Epoch: 34/50..  Training Loss: 66.758..  Test Loss: 2.085..  Test Accuracy: 0.380\n",
            "Epoch: 35/50..  Training Loss: 68.512..  Test Loss: 2.059..  Test Accuracy: 0.417\n",
            "Epoch: 36/50..  Training Loss: 70.268..  Test Loss: 2.069..  Test Accuracy: 0.375\n",
            "Epoch: 37/50..  Training Loss: 72.013..  Test Loss: 2.036..  Test Accuracy: 0.406\n",
            "Epoch: 38/50..  Training Loss: 73.747..  Test Loss: 2.024..  Test Accuracy: 0.443\n",
            "Epoch: 39/50..  Training Loss: 75.473..  Test Loss: 2.055..  Test Accuracy: 0.406\n",
            "Epoch: 40/50..  Training Loss: 77.195..  Test Loss: 2.066..  Test Accuracy: 0.406\n",
            "Epoch: 41/50..  Training Loss: 78.913..  Test Loss: 2.048..  Test Accuracy: 0.406\n",
            "Epoch: 42/50..  Training Loss: 80.628..  Test Loss: 2.046..  Test Accuracy: 0.401\n",
            "Epoch: 43/50..  Training Loss: 82.347..  Test Loss: 2.044..  Test Accuracy: 0.422\n",
            "Epoch: 44/50..  Training Loss: 84.051..  Test Loss: 2.036..  Test Accuracy: 0.422\n",
            "Epoch: 45/50..  Training Loss: 85.747..  Test Loss: 2.041..  Test Accuracy: 0.391\n",
            "Epoch: 46/50..  Training Loss: 87.443..  Test Loss: 2.027..  Test Accuracy: 0.438\n",
            "Epoch: 47/50..  Training Loss: 89.138..  Test Loss: 2.029..  Test Accuracy: 0.427\n",
            "Epoch: 48/50..  Training Loss: 90.827..  Test Loss: 1.992..  Test Accuracy: 0.479\n",
            "Epoch: 49/50..  Training Loss: 92.512..  Test Loss: 2.006..  Test Accuracy: 0.458\n",
            "Epoch: 50/50..  Training Loss: 94.200..  Test Loss: 2.040..  Test Accuracy: 0.406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9z4JBWARDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62632029-c2a6-4453-c0aa-ed8d05dc9503"
      },
      "source": [
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        ps = model(images)\n",
        "        test_loss += criterion(ps, labels).item()\n",
        "\n",
        "        # Accuracy\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "model.train()\n",
        "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
        "      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
        "running_loss = 0"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 2.001..  Test Accuracy: 0.474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK2D5QhVcSvl",
        "colab_type": "text"
      },
      "source": [
        "## Implementing the model using Keras\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ1dFM7UG4lN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64bc1fa6-ead8-4d4f-8611-ad4156ae7fd9"
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.initializers import glorot_uniform\n",
        "np.random.seed(1)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xzhm1-Gkcd13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "outputId": "e60c5597-93c4-4206-8090-0668b1ee8389"
      },
      "source": [
        "maxLen = len(max(X_train, key=len).split())\n",
        "\n",
        "#Embedding Layer\n",
        "vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding (requirement)\n",
        "emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
        "\n",
        "\n",
        "# Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
        "emb_matrix = np.zeros((vocab_len, emb_dim))\n",
        "\n",
        "# Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "for word, index in word_to_index.items():\n",
        "    emb_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "# Define Keras embedding layer with the correct output/input sizes, make it trainable. Use Embedding(...). Make sure to set trainable=False. \n",
        "embedding_layer = Embedding(vocab_len ,emb_dim,trainable = False)\n",
        "\n",
        "# Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
        "embedding_layer.build((None,))\n",
        "\n",
        "# Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
        "embedding_layer.set_weights([emb_matrix])"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 01:31:53.844049 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0809 01:31:53.887402 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0809 01:31:53.913357 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0809 01:31:53.914783 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0809 01:31:53.915848 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Tt0yFSzUMoZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4e9b2c8d-b363-4251-9c29-90af16d0eb83"
      },
      "source": [
        "input_shape = (maxLen,)\n",
        "\n",
        "print(input_shape)\n",
        "\n",
        "# Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
        "sentence_indices = Input(shape=input_shape, dtype='int32')\n",
        "\n",
        "# Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
        "#embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
        "\n",
        "# Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
        "embeddings = embedding_layer(sentence_indices)   \n",
        "\n",
        "print(embeddings.shape)\n",
        "\n",
        "# Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
        "# Be careful, the returned output should be a batch of sequences.\n",
        "X = LSTM(128, return_sequences=True)(embeddings)\n",
        "# Add dropout with a probability of 0.5\n",
        "X =  Dropout(0.5)(X)\n",
        "# Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
        "# Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
        "X = LSTM(128, return_sequences=False)(embeddings)\n",
        "# Add dropout with a probability of 0.5\n",
        "X =  Dropout(0.5)(X)\n",
        "# Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
        "X = Dense(5)(X)\n",
        "# Add a softmax activation\n",
        "X = Activation(activation=\"softmax\")(X)\n",
        "\n",
        "# Create Model instance which converts sentence_indices into X.\n",
        "model = Model(inputs=sentence_indices, outputs=X)"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10,)\n",
            "(?, 10, 50)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0809 01:31:54.817599 140589170952064 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDy1TIaDZ6oF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "1cbd428c-c986-4d3a-80f3-1798d587101b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 10, 50)            20000050  \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (None, 128)               91648     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 5)                 645       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 5)                 0         \n",
            "=================================================================\n",
            "Total params: 20,092,343\n",
            "Trainable params: 92,293\n",
            "Non-trainable params: 20,000,050\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ01cWAFa2yP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "57f48ea3-cdfd-47c5-e84f-6143616569a0"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 01:31:56.079123 140589170952064 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiLCA6vxa5W_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "df8fd820-ee3b-4842-ba32-768bedad931d"
      },
      "source": [
        "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0809 01:32:05.855003 140589170952064 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "132/132 [==============================] - 1s 9ms/step - loss: 1.5826 - acc: 0.2045\n",
            "Epoch 2/50\n",
            "132/132 [==============================] - 0s 530us/step - loss: 1.5320 - acc: 0.2803\n",
            "Epoch 3/50\n",
            "132/132 [==============================] - 0s 537us/step - loss: 1.4892 - acc: 0.3409\n",
            "Epoch 4/50\n",
            "132/132 [==============================] - 0s 605us/step - loss: 1.4576 - acc: 0.3939\n",
            "Epoch 5/50\n",
            "132/132 [==============================] - 0s 579us/step - loss: 1.3974 - acc: 0.4318\n",
            "Epoch 6/50\n",
            "132/132 [==============================] - 0s 557us/step - loss: 1.3166 - acc: 0.4848\n",
            "Epoch 7/50\n",
            "132/132 [==============================] - 0s 517us/step - loss: 1.1831 - acc: 0.6894\n",
            "Epoch 8/50\n",
            "132/132 [==============================] - 0s 544us/step - loss: 1.0381 - acc: 0.6894\n",
            "Epoch 9/50\n",
            "132/132 [==============================] - 0s 545us/step - loss: 0.9883 - acc: 0.6591\n",
            "Epoch 10/50\n",
            "132/132 [==============================] - 0s 571us/step - loss: 0.8912 - acc: 0.6894\n",
            "Epoch 11/50\n",
            "132/132 [==============================] - 0s 552us/step - loss: 0.7674 - acc: 0.7197\n",
            "Epoch 12/50\n",
            "132/132 [==============================] - 0s 544us/step - loss: 0.7384 - acc: 0.7197\n",
            "Epoch 13/50\n",
            "132/132 [==============================] - 0s 615us/step - loss: 0.6776 - acc: 0.7348\n",
            "Epoch 14/50\n",
            "132/132 [==============================] - 0s 519us/step - loss: 0.5327 - acc: 0.8030\n",
            "Epoch 15/50\n",
            "132/132 [==============================] - 0s 559us/step - loss: 0.5603 - acc: 0.7803\n",
            "Epoch 16/50\n",
            "132/132 [==============================] - 0s 550us/step - loss: 0.4446 - acc: 0.8106\n",
            "Epoch 17/50\n",
            "132/132 [==============================] - 0s 549us/step - loss: 0.3746 - acc: 0.8485\n",
            "Epoch 18/50\n",
            "132/132 [==============================] - 0s 646us/step - loss: 0.4197 - acc: 0.8333\n",
            "Epoch 19/50\n",
            "132/132 [==============================] - 0s 507us/step - loss: 0.3655 - acc: 0.8712\n",
            "Epoch 20/50\n",
            "132/132 [==============================] - 0s 441us/step - loss: 0.3391 - acc: 0.8939\n",
            "Epoch 21/50\n",
            "132/132 [==============================] - 0s 433us/step - loss: 0.2952 - acc: 0.9091\n",
            "Epoch 22/50\n",
            "132/132 [==============================] - 0s 476us/step - loss: 0.2399 - acc: 0.9015\n",
            "Epoch 23/50\n",
            "132/132 [==============================] - 0s 537us/step - loss: 0.2225 - acc: 0.9394\n",
            "Epoch 24/50\n",
            "132/132 [==============================] - 0s 518us/step - loss: 0.1774 - acc: 0.9470\n",
            "Epoch 25/50\n",
            "132/132 [==============================] - 0s 498us/step - loss: 0.1825 - acc: 0.9318\n",
            "Epoch 26/50\n",
            "132/132 [==============================] - 0s 461us/step - loss: 0.1824 - acc: 0.9318\n",
            "Epoch 27/50\n",
            "132/132 [==============================] - 0s 526us/step - loss: 0.1438 - acc: 0.9470\n",
            "Epoch 28/50\n",
            "132/132 [==============================] - 0s 519us/step - loss: 0.1068 - acc: 0.9621\n",
            "Epoch 29/50\n",
            "132/132 [==============================] - 0s 553us/step - loss: 0.1378 - acc: 0.9697\n",
            "Epoch 30/50\n",
            "132/132 [==============================] - 0s 547us/step - loss: 0.1947 - acc: 0.9621\n",
            "Epoch 31/50\n",
            "132/132 [==============================] - 0s 530us/step - loss: 0.1944 - acc: 0.9470\n",
            "Epoch 32/50\n",
            "132/132 [==============================] - 0s 444us/step - loss: 0.1927 - acc: 0.9167\n",
            "Epoch 33/50\n",
            "132/132 [==============================] - 0s 431us/step - loss: 0.1416 - acc: 0.9470\n",
            "Epoch 34/50\n",
            "132/132 [==============================] - 0s 462us/step - loss: 0.3011 - acc: 0.9015\n",
            "Epoch 35/50\n",
            "132/132 [==============================] - 0s 425us/step - loss: 0.3042 - acc: 0.8939\n",
            "Epoch 36/50\n",
            "132/132 [==============================] - 0s 414us/step - loss: 0.2716 - acc: 0.9091\n",
            "Epoch 37/50\n",
            "132/132 [==============================] - 0s 479us/step - loss: 0.6491 - acc: 0.8182\n",
            "Epoch 38/50\n",
            "132/132 [==============================] - 0s 413us/step - loss: 0.3556 - acc: 0.8864\n",
            "Epoch 39/50\n",
            "132/132 [==============================] - 0s 414us/step - loss: 0.2546 - acc: 0.9545\n",
            "Epoch 40/50\n",
            "132/132 [==============================] - 0s 460us/step - loss: 0.2642 - acc: 0.9394\n",
            "Epoch 41/50\n",
            "132/132 [==============================] - 0s 424us/step - loss: 0.2429 - acc: 0.9545\n",
            "Epoch 42/50\n",
            "132/132 [==============================] - 0s 429us/step - loss: 0.1608 - acc: 0.9394\n",
            "Epoch 43/50\n",
            "132/132 [==============================] - 0s 440us/step - loss: 0.1499 - acc: 0.9697\n",
            "Epoch 44/50\n",
            "132/132 [==============================] - 0s 408us/step - loss: 0.1070 - acc: 0.9773\n",
            "Epoch 45/50\n",
            "132/132 [==============================] - 0s 435us/step - loss: 0.0777 - acc: 0.9924\n",
            "Epoch 46/50\n",
            "132/132 [==============================] - 0s 416us/step - loss: 0.0659 - acc: 0.9848\n",
            "Epoch 47/50\n",
            "132/132 [==============================] - 0s 423us/step - loss: 0.0395 - acc: 1.0000\n",
            "Epoch 48/50\n",
            "132/132 [==============================] - 0s 447us/step - loss: 0.0404 - acc: 1.0000\n",
            "Epoch 49/50\n",
            "132/132 [==============================] - 0s 439us/step - loss: 0.0350 - acc: 1.0000\n",
            "Epoch 50/50\n",
            "132/132 [==============================] - 0s 451us/step - loss: 0.0270 - acc: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fdcffc31550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rufzBTvUa8Z3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4a62ef2d-e465-415f-ba9c-e96bb10e7702"
      },
      "source": [
        "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
        "print()\n",
        "print(\"Test accuracy = \", acc)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 0s 2ms/step\n",
            "\n",
            "Test accuracy =  0.839285705770765\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "assO_v7x74Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}