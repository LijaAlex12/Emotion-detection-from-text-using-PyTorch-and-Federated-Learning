{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion detection from text using PyTorch and Federated Learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karankishinani/Emotion-detection-from-text-using-PyTorch-and-Federated-Learning/blob/master/Emotion_detection_from_text_using_PyTorch_and_Federated_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rufFNufqDmUO",
        "colab_type": "text"
      },
      "source": [
        "# Emotion detection from text using PyTorch and Federated Learning\n",
        "\n",
        "For this project, we are going to implement an NLP task of creating a model to detect the emotion from text. We will develop this using the PyTorch library and the Federated Learning framework for decentralized training. \n",
        "\n",
        "We will create an emotion detection for the following 5 emotions:\n",
        "\n",
        "| Emotion | Emoji   | Label   |\n",
        "|------|------|------|\n",
        "|Loving| ❤️| 0|\n",
        "|Playful| ⚽️| 1|\n",
        "|Happy| 😄| 2|\n",
        "|Annoyed| 😞| 3|\n",
        "|Foodie| 🍽| 4|\n",
        "\n",
        "## Dataset\n",
        "\n",
        "We will work with a dataset (X, Y) where we have:\n",
        "*   X contains 127 sentences\n",
        "*   Y contains a label between [0, 4] corresponding to the emotion\n",
        "\n",
        "For example:\n",
        "\n",
        "| Sentence | Emotion   |\n",
        "|----------|-----------|\n",
        "|food is life|  🍽 Foodie|\n",
        "|I love you mum|  ❤️ Loving|\n",
        "|Stop saying bullshit|  😞 Annoyed|\n",
        "|congratulations on your acceptance|  😄 Happy|\n",
        "|The assignment is too long|    😞 Annoyed|\n",
        "|I want to go play| ⚽️ Playful|\n",
        "|she did not answer my text| 😞 Annoyed|\n",
        "|Your stupidity has no limit| 😞 Annoyed|\n",
        "|how many points did he score|  ⚽️ Playful|\n",
        "|my algorithm performs poorly| 😞 Annoyed|\n",
        "|I got approved|  😄 Happy|\n",
        "\n",
        "## The Model\n",
        "We will build an LSTM model that takes as input word sequences that will take word ordering into account. We will use 50-dimensional [GloVe](https://nlp.stanford.edu/projects/glove/) pre-trained word embeddings to represent words. We will then feed those as an input into an LSTM that will predict the most appropiate emotion for the text. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1s-KYhU5JWF-jvAlZ2MIKKugxLLDdhpQP)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQn1wO2qr01C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJcTk7DnfXRx",
        "colab_type": "code",
        "outputId": "125f208f-b090-424a-a29c-2ca6727ae990",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d04n5XXyf5Cj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "def read_glove_vecs(glove_file):\n",
        "    with open(glove_file, 'r') as f:\n",
        "        words = set()\n",
        "        word_to_vec_map = {}\n",
        "        for line in f:\n",
        "            line = line.strip().split()\n",
        "            curr_word = line[0]\n",
        "            words.add(curr_word)\n",
        "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
        "        \n",
        "        i = 1\n",
        "        words_to_index = {}\n",
        "        index_to_words = {}\n",
        "        for w in sorted(words):\n",
        "            words_to_index[w] = i\n",
        "            index_to_words[i] = w\n",
        "            i = i + 1\n",
        "    return words_to_index, index_to_words, word_to_vec_map\n",
        "\n",
        "def convert_to_one_hot(Y, C):\n",
        "    Y = np.eye(C)[Y.reshape(-1)]\n",
        "    return Y\n",
        "\n",
        "def read_csv(filename):\n",
        "    phrase = []\n",
        "    emoji = []\n",
        "\n",
        "    with open (filename) as csvDataFile:\n",
        "        csvReader = csv.reader(csvDataFile)\n",
        "\n",
        "        for row in csvReader:\n",
        "            phrase.append(row[0])\n",
        "            emoji.append(row[1])\n",
        "\n",
        "    X = np.asarray(phrase)\n",
        "    Y = np.asarray(emoji, dtype=int)\n",
        "\n",
        "    return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmqjSlwofS_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, Y_train = read_csv('/content/drive/My Drive/Colab Notebooks/data/train_emoji.csv')\n",
        "X_test, Y_test = read_csv('/content/drive/My Drive/Colab Notebooks/data/tesss.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gt5z5eqVih4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
        "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDe5vX3qiYcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('/content/drive/My Drive/Colab Notebooks/data/glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fh0iyxn_jDCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentences_to_indices(X, word_to_index, max_len):\n",
        "    \"\"\"\n",
        "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
        "    \"\"\"\n",
        "    \n",
        "    m = X.shape[0]  # number of training examples\n",
        "    \n",
        "    # Initialize X_indices as a numpy matrix of zeros and the correct shape\n",
        "    X_indices = np.zeros((m,max_len))\n",
        "    \n",
        "    for i in range(m):  # loop over training examples\n",
        "        \n",
        "        # Convert the ith sentence in lower case and split into a list of words\n",
        "        sentence_words = X[i].lower().split()\n",
        "        \n",
        "        # Initialize j to 0\n",
        "        j = 0\n",
        "        \n",
        "        # Loop over the words of sentence_words\n",
        "        for w in sentence_words:\n",
        "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
        "            X_indices[i, j] = word_to_index[w]\n",
        "            # Increment j to j + 1\n",
        "            j = j + 1\n",
        "    \n",
        "    return X_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLyIs_aFjdfl",
        "colab_type": "code",
        "outputId": "31b1e03b-5781-4744-fc0a-bd6f930ba584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "X1 = np.array([\"lol\", \"I love you\", \"this is very yummy\"])\n",
        "X1_indices = sentences_to_indices(X1,word_to_index, max_len = 5)\n",
        "print(\"X1 =\", X1)\n",
        "print(\"X1_indices =\", X1_indices)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X1 = ['lol' 'I love you' 'this is very yummy']\n",
            "X1_indices = [[225122.      0.      0.      0.      0.]\n",
            " [185457. 226278. 394475.      0.      0.]\n",
            " [358160. 192973. 377946. 394957.      0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx57hw8Pjg3q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMKHhghrWcNn",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Network using Pretrained Embedding Layer using GloVe Word Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZDWRfRkWhwB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pretrained_embedding_layer(word_to_vec_map, word_to_index, non_trainable=True):\n",
        "    num_embeddings = len(word_to_index) + 1                   \n",
        "    embedding_dim = word_to_vec_map[\"cucumber\"].shape[0]  #  dimensionality of GloVe word vectors (= 50)\n",
        "\n",
        "    # Initialize the embedding matrix as a numpy array of zeros of shape (num_embeddings, embedding_dim)\n",
        "    weights_matrix = np.zeros((num_embeddings, embedding_dim))\n",
        "\n",
        "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
        "    for word, index in word_to_index.items():\n",
        "        weights_matrix[index, :] = word_to_vec_map[word]\n",
        "\n",
        "    emb_layer = nn.Embedding.from_pretrained(torch.tensor(weights_matrix).type(torch.FloatTensor))\n",
        "\n",
        "    if non_trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "\n",
        "    return emb_layer, num_embeddings, embedding_dim\n",
        "\n",
        "class NN(nn.Module):\n",
        "    def __init__(self, word_to_vec_map, word_to_index):\n",
        "        super(NN, self).__init__()\n",
        "        \n",
        "        self.embedding, embed_size, embedding_dim = pretrained_embedding_layer(word_to_vec_map, word_to_index, True)\n",
        "        self.hidden_size = 128\n",
        "        self.num_layers = 2\n",
        "        self.output_size = 5\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=self.hidden_size, num_layers=self.num_layers,\n",
        "                                batch_first=True, dropout=0.5)\n",
        "        self.fc1 = nn.Linear(self.hidden_size, self.output_size)\n",
        "        \n",
        "    def forward(self, sentence):\n",
        "        x = self.embedding(sentence.type(torch.LongTensor))\n",
        "        x, _ = self.lstm(x)\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.fc1(x)\n",
        "        x = F.softmax(x)\n",
        "        return x\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEkK7S6v8dVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train(model, trainloader, criterion, optimizer, epochs=10):\n",
        "    \n",
        "    model.to(device)\n",
        "    running_loss = 0\n",
        "    \n",
        "    for e in range(epochs):\n",
        "        \n",
        "        model.train()\n",
        "        \n",
        "        for images, labels in trainloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # 1) erase previous gradients (if they exist)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2) make a prediction\n",
        "            pred = model.forward(images)\n",
        "\n",
        "            # 3) calculate how much we missed\n",
        "            loss = criterion(pred, labels)\n",
        "\n",
        "            # 4) figure out which weights caused us to miss\n",
        "            loss.backward()\n",
        "\n",
        "            # 5) change those weights\n",
        "            optimizer.step()\n",
        "\n",
        "            # 6) log our progress\n",
        "            running_loss += loss.item()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fgb8si4s9Jb9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "outputId": "8da49194-6c2b-47d4-a12a-7a161674e91a"
      },
      "source": [
        "import torch.utils.data\n",
        "\n",
        "maxLen = len(max(X_train, key=len).split())\n",
        "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
        "Y_train_oh = convert_to_one_hot(Y_train, C = 5)\n",
        "\n",
        "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
        "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
        "\n",
        "model = NN(word_to_vec_map, word_to_index)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "epochs = 50\n",
        "batch_size = 32\n",
        "train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train_indices), torch.tensor(Y_train_oh).type(torch.LongTensor))\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test_indices), torch.tensor(Y_test_oh).type(torch.LongTensor))\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "train(model, train_loader, criterion, optimizer, epochs)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vo9z4JBWARDf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "252d9284-899b-4f1d-b891-6db71870930a"
      },
      "source": [
        "test_loss = 0\n",
        "accuracy = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        ps = model(images)\n",
        "        test_loss += criterion(ps, labels).item()\n",
        "\n",
        "        # Accuracy\n",
        "        top_p, top_class = ps.topk(1, dim=1)\n",
        "        equals = top_class == labels.view(*top_class.shape)\n",
        "        accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
        "model.train()\n",
        "print(\"Test Loss: {:.3f}.. \".format(test_loss/len(test_loader)),\n",
        "      \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n",
        "running_loss = 0"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 2.292..  Test Accuracy: 0.206\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ1dFM7UG4lN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}